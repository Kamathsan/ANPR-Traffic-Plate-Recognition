{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ANPR-Traffic-Plate-Recognition"
      ],
      "metadata": {
        "id": "h8Z_DktDltx-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10osEhPz5Bg_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics easyocr opencv-python roboflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"Ft4uvDzqztrfLBv5grnn\")  # Get your free key in 30 seconds at https://app.roboflow.com\n",
        "project = rf.workspace(\"roboflow-universe-projects\").project(\"license-plate-recognition-rxg4e\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjoJcrpN5WKj",
        "outputId": "63300fee-678b-4bb7-8ad5-df052688ee56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in License-Plate-Recognition-4 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 974784/974784 [00:51<00:00, 18806.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to License-Plate-Recognition-4 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48488/48488 [00:06<00:00, 7039.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=30 imgsz=640 batch=16 plots=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUZuqhcU5_N3",
        "outputId": "0180587b-dd11-4bfc-ac45-f987b32bc0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 21.5MB 238.1MB/s 0.1s\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 21.5MB 237.7MB/s 0.1s\n",
            "Ultralytics 8.3.229 ðŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/License-Plate-Recognition-4/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 116.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 103.4MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1542.5Â±447.9 MB/s, size: 43.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/License-Plate-Recognition-4/train/labels... 21173 images, 28 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 21173/21173 2.3Kit/s 9.3s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/License-Plate-Recognition-4/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 624.9Â±319.0 MB/s, size: 39.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/License-Plate-Recognition-4/valid/labels... 2046 images, 3 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2046/2046 929.5it/s 2.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/License-Plate-Recognition-4/valid/labels.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      3.75G      1.304     0.9924      1.308          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.1it/s 7:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.3it/s 19.3s\n",
            "                   all       2046       2132      0.922       0.85      0.917      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      4.54G      1.297     0.7909      1.308          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.2it/s 6:52\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.3s\n",
            "                   all       2046       2132      0.901       0.86      0.917       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      4.58G      1.264      0.752      1.287          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:46\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.8s\n",
            "                   all       2046       2132      0.956      0.919      0.953       0.61\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      4.62G      1.231     0.6998      1.264          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.6s\n",
            "                   all       2046       2132      0.929      0.927      0.947      0.627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      4.65G      1.205     0.6559      1.245          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:44\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 19.0s\n",
            "                   all       2046       2132      0.958      0.923      0.965       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      4.69G      1.184      0.631      1.232          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.9s\n",
            "                   all       2046       2132      0.959      0.931      0.963      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      4.73G      1.168     0.6106      1.215         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:39\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.9s\n",
            "                   all       2046       2132      0.973       0.94      0.975      0.649\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      4.76G      1.147     0.5971      1.207          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 19.0s\n",
            "                   all       2046       2132      0.965      0.945      0.972      0.654\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30       4.8G      1.134     0.5792      1.197         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.2s\n",
            "                   all       2046       2132      0.975      0.952      0.978      0.662\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      4.84G      1.122     0.5655       1.19         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.2s\n",
            "                   all       2046       2132      0.976      0.952      0.978      0.662\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      4.87G      1.114     0.5552      1.185          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.2s\n",
            "                   all       2046       2132      0.979      0.946      0.978      0.673\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      4.91G      1.098     0.5396      1.177          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:39\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.9s\n",
            "                   all       2046       2132      0.981       0.95      0.979      0.677\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      4.94G      1.086     0.5299      1.167          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.8s\n",
            "                   all       2046       2132      0.977      0.958       0.98       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      4.98G      1.083     0.5292       1.17          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:39\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.5s\n",
            "                   all       2046       2132      0.968      0.951      0.977      0.683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      5.02G      1.072     0.5134      1.155         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.6it/s 17.6s\n",
            "                   all       2046       2132      0.974      0.952      0.978      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      5.06G      1.059     0.5007      1.149         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:39\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.6it/s 18.0s\n",
            "                   all       2046       2132      0.974      0.955      0.982      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      5.09G      1.049     0.4921      1.147          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.2s\n",
            "                   all       2046       2132      0.975      0.958      0.981      0.687\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      5.13G      1.041     0.4841      1.141          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.9s\n",
            "                   all       2046       2132      0.965      0.966      0.982      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      5.16G      1.034     0.4805      1.135          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.8s\n",
            "                   all       2046       2132      0.986       0.95      0.984      0.696\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30       5.2G      1.024       0.47      1.128          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.8s\n",
            "                   all       2046       2132      0.973      0.962      0.981      0.694\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      5.23G      1.012     0.4014      1.159          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.3it/s 6:35\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.6it/s 18.0s\n",
            "                   all       2046       2132      0.988      0.953      0.985      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      5.27G     0.9947     0.3873      1.148          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.4it/s 6:33\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.4it/s 18.9s\n",
            "                   all       2046       2132      0.986      0.956      0.985      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      5.31G     0.9813     0.3816      1.146          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.4it/s 6:33\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.6it/s 18.0s\n",
            "                   all       2046       2132      0.983      0.956      0.985      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      5.34G     0.9683     0.3751      1.135          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.4it/s 6:32\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.1s\n",
            "                   all       2046       2132      0.983      0.959      0.986      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      5.38G     0.9623     0.3686      1.134          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1324/1324 3.4it/s 6:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 64/64 3.5it/s 18.5s\n",
            "                   all       2046       2132      0.988      0.954      0.986      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      5.39G     0.9451     0.3579      1.122         16        640: 69% â”â”â”â”â”â”â”â”â”€â”€â”€â”€ 915/1324 3.6it/s 4:32<1:52"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will show you exactly where your model was saved\n",
        "!ls -la /content/runs/detect/train/weights/"
      ],
      "metadata": {
        "id": "RAbUc8Nipo8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Automatically find the latest training folder\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Find the latest 'train' folder\n",
        "train_dirs = sorted(glob.glob(\"/content/runs/detect/train*\"))\n",
        "latest_train = train_dirs[-1] if train_dirs else \"/content/runs/detect/train\"\n",
        "\n",
        "model_path = os.path.join(latest_train, \"weights/best.pt\")\n",
        "\n",
        "# Load the model\n",
        "model = YOLO(model_path)\n",
        "print(f\"Model loaded successfully from: {model_path}\")\n"
      ],
      "metadata": {
        "id": "SN1G30I_qaMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "id": "sH9sDX-aqgTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = \"/content/License-Plate-Recognition-4\"   # â† CHANGE THIS to whatever you saw above\n",
        "\n",
        "test_images_path = os.path.join(dataset_path, \"test/images\")\n",
        "print(\"Looking for test images here:\", test_images_path)\n",
        "!ls {test_images_path} | head -5   # This should show some .jpg files"
      ],
      "metadata": {
        "id": "Z8tTAompq2i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the correct folder you found\n",
        "correct_folder = \"License-Plate-Recognition-4\"   # â† PUT YOUR ACTUAL FOLDER NAME HERE\n",
        "\n",
        "# Test prediction (this will work now)\n",
        "results = model.predict(\n",
        "    source=f\"/content/{correct_folder}/test/images\",\n",
        "    save=True,\n",
        "    conf=0.5,\n",
        "    project=\"/content/results\"\n",
        ")\n",
        "\n",
        "print(\"Done! Check the folder /content/results/predict for output images with detected plates\")"
      ],
      "metadata": {
        "id": "X4RE4fF6q7VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-detect the real dataset folder and run prediction\n",
        "import glob, os\n",
        "dataset_folder = glob.glob(\"/content/*Plate*\")[0]  # finds any folder with \"Plate\" in name\n",
        "model.predict(source=f\"{dataset_folder}/test/images\", save=True, conf=0.5)\n",
        "!echo \"Check results here:\" && ls /content/runs/detect/predict*"
      ],
      "metadata": {
        "id": "a23-mvAarNdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import easyocr\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "# Initialize OCR reader (only once)\n",
        "reader = easyocr.Reader(['en'], gpu=True)  # Add 'hi' for Hindi/Indian plates if needed\n",
        "\n",
        "# Auto-find your dataset folder\n",
        "dataset_folder = glob.glob(\"/content/*Plate*\")[0]\n",
        "test_images = glob.glob(f\"{dataset_folder}/test/images/*.jpg\")[:10]  # First 10 images\n",
        "\n",
        "for img_path in test_images:\n",
        "    img = cv2.imread(img_path)\n",
        "    results = model(img, conf=0.5)[0]\n",
        "\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        confidence = box.conf[0]\n",
        "\n",
        "        # Crop the plate\n",
        "        plate_crop = img[y1:y2, x1:x2]\n",
        "\n",
        "        # OCR\n",
        "        ocr_result = reader.readtext(plate_crop, allowlist='0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ-', detail=0)\n",
        "        plate_text = ocr_result[0] if ocr_result else \"??????\"\n",
        "\n",
        "        # Draw everything\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(img, f\"{plate_text} ({confidence:.2f})\", (x1, y1-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    cv2_imshow(img)\n",
        "    print(f\"Detected: {plate_text}\\n\")"
      ],
      "metadata": {
        "id": "ApASRQ6erck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List files to confirm\n",
        "!ls -lh \"Traffic Control CCTV.mp4\"\n",
        "\n",
        "# Move/rename it so we can use a clean name (optional but recommended)\n",
        "!cp \"Traffic Control CCTV.mp4\" traffic_input.mp4"
      ],
      "metadata": {
        "id": "tfXjbUVBrpYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import easyocr\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize OCR (only once)\n",
        "reader = easyocr.Reader(['en'], gpu=True)   # change to ['hi','en'] if Indian plates\n",
        "\n",
        "# Open your uploaded video\n",
        "cap = cv2.VideoCapture(\"traffic_input.mp4\")\n",
        "\n",
        "# Get video properties\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f\"Video loaded: {width}Ã—{height}, {fps:.2f} FPS, {total_frames} frames\")\n",
        "\n",
        "# Output video\n",
        "out = cv2.VideoWriter('MY_ANPR_RESULT.mp4',\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      fps, (width, height))\n",
        "\n",
        "frame_idx = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # YOLO detection\n",
        "    results = model(frame, conf=0.35, iou=0.5, verbose=False)[0]  # lowered conf a bit for real CCTV\n",
        "\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        conf = box.conf.item()\n",
        "\n",
        "        # Crop plate region\n",
        "        plate = frame[y1:y2, x1:x2]\n",
        "        if plate.size == 0:\n",
        "            continue\n",
        "\n",
        "        # OCR (tuned for real-world plates)\n",
        "        text_list = reader.readtext(plate,\n",
        "                                    allowlist='0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ- ',\n",
        "                                    detail=0,\n",
        "                                    paragraph=False,\n",
        "                                    width_ths=0.8,\n",
        "                                    height_ths=0.8)\n",
        "        plate_text = text_list[0].upper() if text_list else \"???\"\n",
        "\n",
        "        # Draw results\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(frame, plate_text, (x1, y1-15),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 3)\n",
        "        cv2.putText(frame, f\"{conf:.2f}\", (x1, y2+30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_idx += 1\n",
        "    if frame_idx % 50 == 0 or frame_idx == total_frames:\n",
        "        print(f\"Processed {frame_idx}/{total_frames} frames...\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\"FINISHED! Your result video â†’ MY_ANPR_RESULT.mp4\")"
      ],
      "metadata": {
        "id": "f_VdktQ7sN2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('MY_ANPR_RESULT.mp4')"
      ],
      "metadata": {
        "id": "dnUMmPSGtXF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}